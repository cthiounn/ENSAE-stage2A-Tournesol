{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "class ScoreMode():\n",
    "    DEFAULT = \"default\"\n",
    "    ALL_EQUAL = \"all_equal\"\n",
    "    TRUSTED_ONLY = \"trusted_only\"\n",
    "    \n",
    "EPSILON = 1e-6  # convergence tolerance\n",
    "\n",
    "\n",
    "def QrMed(W: float, w: Union[pd.Series, float], x: pd.Series, delta: pd.Series):\n",
    "    \"\"\"\n",
    "    Quadratically regularized median\n",
    "\n",
    "    Parameters:\n",
    "        * `W`: Byzantine resilience parameter.\n",
    "            The influence of a single contributor 'i' is bounded by (w_i/W)\n",
    "        * `w`: voting rights vector\n",
    "        * `x`: partial scores vector\n",
    "        * `delta`: partial scores uncertainties vector\n",
    "    \"\"\"\n",
    "    if len(x) == 0:\n",
    "        return 0.0\n",
    "    if isinstance(w, pd.Series):\n",
    "        w = w.to_numpy()\n",
    "    if isinstance(x, pd.Series):\n",
    "        x = x.to_numpy()\n",
    "    if isinstance(delta, pd.Series):\n",
    "        delta = delta.to_numpy()\n",
    "    delta_2 = delta ** 2\n",
    "\n",
    "    def L_prime(m: float):\n",
    "        x_minus_m = x - m\n",
    "        return W * m - np.sum(w * x_minus_m / np.sqrt(delta_2 + x_minus_m ** 2))\n",
    "\n",
    "    m_low = -1.0\n",
    "    while L_prime(m_low) > 0:\n",
    "        m_low *= 2\n",
    "\n",
    "    m_up = 1.0\n",
    "    while L_prime(m_up) < 0:\n",
    "        m_up *= 2\n",
    "\n",
    "    # Brent’s method is used as a faster alternative to usual bisection\n",
    "    return brentq(L_prime, m_low, m_up, xtol=EPSILON)\n",
    "\n",
    "\n",
    "def QrDev(\n",
    "    W: float,\n",
    "    default_dev: float,\n",
    "    w: Union[pd.Series, float],\n",
    "    x: pd.Series,\n",
    "    delta: pd.Series,\n",
    "    qr_med=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Quadratically regularized deviation, between x and their QrMed.\n",
    "    Can be understood as a measure of polarization.\n",
    "    \"\"\"\n",
    "    if qr_med is None:\n",
    "        qr_med = QrMed(W, w, x, delta)\n",
    "    return default_dev + QrMed(W, w, np.abs(x - qr_med) - default_dev, delta)\n",
    "\n",
    "\n",
    "def QrUnc(\n",
    "    W: float,\n",
    "    default_dev: float,\n",
    "    w: pd.Series,\n",
    "    x: pd.Series,\n",
    "    delta: pd.Series,\n",
    "    qr_med=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Quadratically regularized uncertainty\n",
    "    \"\"\"\n",
    "    if isinstance(w, pd.Series):\n",
    "        w = w.to_numpy()\n",
    "    if isinstance(x, pd.Series):\n",
    "        x = x.to_numpy()\n",
    "    if isinstance(delta, pd.Series):\n",
    "        delta = delta.to_numpy()\n",
    "\n",
    "    if qr_med is None:\n",
    "        qr_med = QrMed(W, w, x, delta)\n",
    "    qr_dev = QrDev(W, default_dev, w, x, delta, qr_med=qr_med)\n",
    "    delta_2 = delta ** 2\n",
    "    h = W + np.sum(\n",
    "        w * np.minimum(1, delta_2 * (delta_2 + (x - qr_med) ** 2) ** (-3 / 2))\n",
    "    )\n",
    "\n",
    "    if h <= W:\n",
    "        return qr_dev\n",
    "\n",
    "    k = (h - W) ** (-1 / 2)\n",
    "    return (np.exp(-qr_dev) * qr_dev + np.exp(-k) * k) / (np.exp(-qr_dev) + np.exp(-k))\n",
    "\n",
    "\n",
    "def Clip(x: np.ndarray, center: float, radius: float):\n",
    "    return x.clip(center - radius, center + radius)\n",
    "\n",
    "\n",
    "def ClipMean(w: np.ndarray, x: np.ndarray, center: float, radius: float):\n",
    "    return np.sum(w * Clip(x, center, radius)) / np.sum(w)\n",
    "\n",
    "\n",
    "def BrMean(W: float, w: Union[float, np.ndarray], x: np.ndarray, delta: np.ndarray):\n",
    "    \"\"\"\n",
    "    Byzantine-robustified mean\n",
    "    \"\"\"\n",
    "    if len(x) == 0:\n",
    "        return 0.0\n",
    "    if isinstance(w, float):\n",
    "        w = np.full(x.shape, w)\n",
    "    return ClipMean(w, x, center=QrMed(4 * W, w, x, delta), radius=np.sum(w) / (4 * W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MlInput(ABC):\n",
    "    @abstractmethod\n",
    "    def get_comparisons(\n",
    "        self,\n",
    "        trusted_only=False,\n",
    "        criteria: Optional[str] = None,\n",
    "        user_id: Optional[int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Fetch data about comparisons submitted by users\n",
    "\n",
    "        Returns:\n",
    "        - comparisons_df: DataFrame with columns\n",
    "            * `user_id`: int\n",
    "            * `entity_a`: int or str\n",
    "            * `entity_b`: int or str\n",
    "            * `criteria`: str\n",
    "            * `score`: float\n",
    "            * `weight`: float\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_ratings_properties(self) -> pd.DataFrame:\n",
    "        \"\"\"Fetch data about contributor ratings properties\n",
    "\n",
    "        Returns:\n",
    "        - ratings_df: DataFrame with columns\n",
    "            * `user_id`: int\n",
    "            * `entity_id`: int or str\n",
    "            * `is_public`: bool\n",
    "            * `is_trusted`: bool\n",
    "            * `is_supertrusted`: bool\n",
    "        \"\"\"\n",
    "        pass\n",
    "class MlInputFromPublicDataset(MlInput):\n",
    "    def __init__(self, csv_file):\n",
    "        self.public_dataset = pd.read_csv(csv_file)\n",
    "        self.public_dataset.rename(\n",
    "            {\"video_a\": \"entity_a\", \"video_b\": \"entity_b\"}, axis=1, inplace=True\n",
    "        )\n",
    "        self.public_dataset[\"user_id\"], self.user_indices = self.public_dataset[\n",
    "            \"public_username\"\n",
    "        ].factorize()\n",
    "\n",
    "    def get_comparisons(\n",
    "        self, trusted_only=False, criteria=None, user_id=None\n",
    "    ) -> pd.DataFrame:\n",
    "        df = self.public_dataset.copy(deep=False)\n",
    "        if criteria is not None:\n",
    "            df = df[df.criteria == criteria]\n",
    "        if user_id is not None:\n",
    "            df = df[df.user_id == user_id]\n",
    "        return df[[\"user_id\", \"entity_a\", \"entity_b\", \"criteria\", \"score\", \"weight\"]]\n",
    "\n",
    "    def get_ratings_properties(self):\n",
    "        user_entities_pairs = pd.Series(\n",
    "            iter(\n",
    "                set(self.public_dataset.groupby([\"user_id\", \"entity_a\"]).indices.keys())\n",
    "                | set(\n",
    "                    self.public_dataset.groupby([\"user_id\", \"entity_b\"]).indices.keys()\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        df = pd.DataFrame([*user_entities_pairs], columns=[\"user_id\", \"entity_id\"])\n",
    "        df[\"is_public\"] = True\n",
    "        top_users = df.value_counts(\"user_id\").index[:6]\n",
    "        df[\"is_trusted\"] = df[\"is_supertrusted\"] = df[\"user_id\"].isin(top_users)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 20.0\n",
    "\n",
    "SCALING_WEIGHT_SUPERTRUSTED = W\n",
    "SCALING_WEIGHT_TRUSTED = 1.0\n",
    "SCALING_WEIGHT_NONTRUSTED = 0.0\n",
    "\n",
    "VOTE_WEIGHT_TRUSTED_PUBLIC = 1.0\n",
    "VOTE_WEIGHT_TRUSTED_PRIVATE = 0.5\n",
    "\n",
    "TOTAL_VOTE_WEIGHT_NONTRUSTED_DEFAULT = 2.0  # w_⨯,default\n",
    "TOTAL_VOTE_WEIGHT_NONTRUSTED_FRACTION = 0.1  # f_⨯\n",
    "\n",
    "def get_global_scores(scaled_individual_scores: pd.DataFrame, score_mode: ScoreMode):\n",
    "    df = scaled_individual_scores.copy(deep=False)\n",
    "\n",
    "    if score_mode == ScoreMode.TRUSTED_ONLY:\n",
    "        df = df[df[\"is_trusted\"]]\n",
    "        df[\"voting_weight\"] = 1\n",
    "\n",
    "    if score_mode == ScoreMode.ALL_EQUAL:\n",
    "        df[\"voting_weight\"] = 1\n",
    "\n",
    "    if score_mode == ScoreMode.DEFAULT:\n",
    "        # Voting weight for non trusted users will be computed per entity\n",
    "        df[\"voting_weight\"] = 0\n",
    "        df[\"voting_weight\"].mask(\n",
    "            (df.is_trusted) & (df.is_public),\n",
    "            VOTE_WEIGHT_TRUSTED_PUBLIC,\n",
    "            inplace=True,\n",
    "        )\n",
    "        df[\"voting_weight\"].mask(\n",
    "            (df.is_trusted) & (~df.is_public),\n",
    "            VOTE_WEIGHT_TRUSTED_PRIVATE,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    global_scores = {}\n",
    "    for (entity_id, scores) in df.groupby(\"entity_id\"):\n",
    "        if score_mode == ScoreMode.DEFAULT:\n",
    "            trusted_weight = scores[\"voting_weight\"].sum()\n",
    "            non_trusted_weight = (\n",
    "                TOTAL_VOTE_WEIGHT_NONTRUSTED_DEFAULT\n",
    "                + TOTAL_VOTE_WEIGHT_NONTRUSTED_FRACTION * trusted_weight\n",
    "            )\n",
    "            nb_non_trusted_public = (\n",
    "                scores[\"is_public\"] & (~scores[\"is_trusted\"])\n",
    "            ).sum()\n",
    "            nb_non_trusted_private = (\n",
    "                ~scores[\"is_public\"] & (~scores[\"is_trusted\"])\n",
    "            ).sum()\n",
    "\n",
    "            if (nb_non_trusted_private > 0) or (nb_non_trusted_public > 0):\n",
    "                scores[\"voting_weight\"].mask(\n",
    "                    scores[\"is_public\"] & (scores[\"voting_weight\"] == 0),\n",
    "                    min(\n",
    "                        VOTE_WEIGHT_TRUSTED_PUBLIC,\n",
    "                        2\n",
    "                        * non_trusted_weight\n",
    "                        / (2 * nb_non_trusted_public + nb_non_trusted_private),\n",
    "                    ),\n",
    "                    inplace=True,\n",
    "                )\n",
    "                scores[\"voting_weight\"].mask(\n",
    "                    ~scores[\"is_public\"] & (scores[\"voting_weight\"] == 0),\n",
    "                    min(\n",
    "                        VOTE_WEIGHT_TRUSTED_PRIVATE,\n",
    "                        non_trusted_weight\n",
    "                        / (2 * nb_non_trusted_public + nb_non_trusted_private),\n",
    "                    ),\n",
    "                    inplace=True,\n",
    "                )\n",
    "\n",
    "        w = scores.voting_weight\n",
    "        theta = scores.score\n",
    "        delta = scores.uncertainty\n",
    "        rho = QrMed(2 * W, w, theta, delta)\n",
    "        rho_uncertainty = QrUnc(2 * W, 1, w, theta, delta, qr_med=rho)\n",
    "        rho_deviation = QrDev(2 * W, 1, w, theta, delta, qr_med=rho)\n",
    "        global_scores[entity_id] = {\n",
    "            \"score\": rho,\n",
    "            \"uncertainty\": rho_uncertainty,\n",
    "            \"deviation\": rho_deviation,\n",
    "        }\n",
    "\n",
    "    if len(global_scores) == 0:\n",
    "        return pd.DataFrame(columns=[\"entity_id\", \"score\", \"uncertainty\", \"deviation\"])\n",
    "\n",
    "    result = pd.DataFrame.from_dict(global_scores, orient=\"index\")\n",
    "    result.index.name = \"entity_id\"\n",
    "    return result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_scaling_weights(ml_input: MlInput):\n",
    "    ratings_properties = ml_input.get_ratings_properties()[\n",
    "        [\"user_id\", \"is_trusted\", \"is_supertrusted\"]\n",
    "    ]\n",
    "    df = ratings_properties.groupby(\"user_id\").first()\n",
    "    df[\"scaling_weight\"] = SCALING_WEIGHT_NONTRUSTED\n",
    "    df[\"scaling_weight\"].mask(\n",
    "        df.is_trusted,\n",
    "        SCALING_WEIGHT_TRUSTED,\n",
    "        inplace=True,\n",
    "    )\n",
    "    df[\"scaling_weight\"].mask(\n",
    "        df.is_supertrusted,\n",
    "        SCALING_WEIGHT_SUPERTRUSTED,\n",
    "        inplace=True,\n",
    "    )\n",
    "    return df[\"scaling_weight\"].to_dict()\n",
    "\n",
    "def get_significantly_different_pairs(scores: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Find the set of pairs of alternatives\n",
    "    that are significantly different, according to the contributor scores.\n",
    "    (Used for collaborative preference scaling)\n",
    "    \"\"\"\n",
    "    scores = scores[[\"uid\", \"score\", \"uncertainty\"]]\n",
    "    left, right = np.triu_indices(len(scores), k=1)\n",
    "    pairs = (\n",
    "        scores.iloc[left]\n",
    "        .reset_index(drop=True)\n",
    "        .join(\n",
    "            scores.iloc[right].reset_index(drop=True),\n",
    "            lsuffix=\"_a\",\n",
    "            rsuffix=\"_b\",\n",
    "        )\n",
    "    )\n",
    "    pairs.set_index([\"uid_a\", \"uid_b\"], inplace=True)\n",
    "    return pairs.loc[\n",
    "        np.abs(pairs.score_a - pairs.score_b)\n",
    "        >= 2 * (pairs.uncertainty_a + pairs.uncertainty_b)\n",
    "    ]\n",
    "\n",
    "def compute_scaling(\n",
    "    df: pd.DataFrame,\n",
    "    ml_input: MlInput,\n",
    "    users_to_compute=None,\n",
    "    reference_users=None,\n",
    "    compute_uncertainties=False,\n",
    "):\n",
    "    scaling_weights = get_user_scaling_weights(ml_input)\n",
    "    df = df.rename({\"entity_id\": \"uid\"}, axis=1)\n",
    "\n",
    "    if users_to_compute is None:\n",
    "        users_to_compute = set(df.user_id.unique())\n",
    "    else:\n",
    "        users_to_compute = set(users_to_compute)\n",
    "\n",
    "    if reference_users is None:\n",
    "        reference_users = set(df.user_id.unique())\n",
    "    else:\n",
    "        reference_users = set(reference_users)\n",
    "\n",
    "    s_dict = {}\n",
    "    delta_s_dict = {}\n",
    "\n",
    "    for (user_n, user_scores) in df[df.user_id.isin(users_to_compute)].groupby(\n",
    "        \"user_id\"\n",
    "    ):\n",
    "        s_nqm = []\n",
    "        delta_s_nqm = []\n",
    "        s_weights = []\n",
    "\n",
    "        ABn_all = get_significantly_different_pairs(user_scores)\n",
    "        user_scores_uids = set(ABn_all.index.get_level_values(\"uid_a\")) | set(\n",
    "            ABn_all.index.get_level_values(\"uid_b\")\n",
    "        )\n",
    "\n",
    "        for (user_m, m_scores) in df[\n",
    "            df.user_id.isin(reference_users - {user_n})\n",
    "        ].groupby(\"user_id\"):\n",
    "            common_uids = user_scores_uids.intersection(m_scores.uid)\n",
    "\n",
    "            if len(common_uids) == 0:\n",
    "                continue\n",
    "\n",
    "            m_scores = m_scores[m_scores.uid.isin(common_uids)]\n",
    "            ABm = get_significantly_different_pairs(m_scores)\n",
    "            ABnm = ABn_all.join(ABm, how=\"inner\", lsuffix=\"_n\", rsuffix=\"_m\")\n",
    "            if len(ABnm) == 0:\n",
    "                continue\n",
    "            s_nqmab = np.abs(ABnm.score_a_m - ABnm.score_b_m) / np.abs(\n",
    "                ABnm.score_a_n - ABnm.score_b_n\n",
    "            )\n",
    "\n",
    "            delta_s_nqmab = (\n",
    "                (\n",
    "                    np.abs(ABnm.score_a_m - ABnm.score_b_m)\n",
    "                    + ABnm.uncertainty_a_m\n",
    "                    + ABnm.uncertainty_b_m\n",
    "                )\n",
    "                / (\n",
    "                    np.abs(ABnm.score_a_n - ABnm.score_b_n)\n",
    "                    - ABnm.uncertainty_a_n\n",
    "                    - ABnm.uncertainty_b_n\n",
    "                )\n",
    "            ) - s_nqmab\n",
    "\n",
    "            s = QrMed(1, 1, s_nqmab, delta_s_nqmab)\n",
    "            s_nqm.append(s)\n",
    "            delta_s_nqm.append(QrUnc(1, 1, 1, s_nqmab, delta_s_nqmab, qr_med=s))\n",
    "            s_weights.append(scaling_weights[user_m])\n",
    "\n",
    "        s_weights = np.array(s_weights)\n",
    "        theta_inf = np.max(user_scores.score.abs())\n",
    "        s_nqm = np.array(s_nqm)\n",
    "        delta_s_nqm = np.array(delta_s_nqm)\n",
    "        if compute_uncertainties:\n",
    "            qr_med = QrMed(8 * W * theta_inf, s_weights, s_nqm - 1, delta_s_nqm)\n",
    "            s_dict[user_n] = 1 + qr_med\n",
    "            delta_s_dict[user_n] = QrUnc(\n",
    "                8 * W * theta_inf, 1, s_weights, s_nqm - 1, delta_s_nqm, qr_med=qr_med\n",
    "            )\n",
    "        else:\n",
    "            # When dealing with a sufficiently trustworthy set of users\n",
    "            # and we don't need to compute uncertainties, `BrMean`can be used\n",
    "            # to be closer to the \"sparse unanimity conditions\" discussed in\n",
    "            # [Robust sparse voting](https://arxiv.org/abs/2202.08656)\n",
    "            s_dict[user_n] = 1 + BrMean(\n",
    "                8 * W * theta_inf, s_weights, s_nqm - 1, delta_s_nqm\n",
    "            )\n",
    "\n",
    "    tau_dict = {}\n",
    "    delta_tau_dict = {}\n",
    "    for (user_n, user_scores) in df[df.user_id.isin(users_to_compute)].groupby(\n",
    "        \"user_id\"\n",
    "    ):\n",
    "        tau_nqm = []\n",
    "        delta_tau_nqm = []\n",
    "        s_weights = []\n",
    "        for (user_m, m_scores) in df[\n",
    "            df.user_id.isin(reference_users - {user_n})\n",
    "        ].groupby(\"user_id\"):\n",
    "            common_uids = list(set(user_scores.uid).intersection(m_scores.uid))\n",
    "\n",
    "            if len(common_uids) == 0:\n",
    "                continue\n",
    "\n",
    "            m_scores = m_scores.set_index(\"uid\").loc[common_uids]\n",
    "            n_scores = user_scores.set_index(\"uid\").loc[common_uids]\n",
    "\n",
    "            tau_nqmab = (\n",
    "                s_dict.get(user_m, 1) * m_scores.score - s_dict[user_n] * n_scores.score\n",
    "            )\n",
    "            delta_tau_nqmab = (\n",
    "                s_dict[user_n] * n_scores.uncertainty\n",
    "                + s_dict.get(user_m, 1) * m_scores.uncertainty\n",
    "            )\n",
    "\n",
    "            tau = QrMed(1, 1, tau_nqmab, delta_tau_nqmab)\n",
    "            tau_nqm.append(tau)\n",
    "            delta_tau_nqm.append(QrUnc(1, 1, 1, tau_nqmab, delta_tau_nqmab, qr_med=tau))\n",
    "            s_weights.append(scaling_weights[user_m])\n",
    "\n",
    "        s_weights = np.array(s_weights)\n",
    "        tau_nqm = np.array(tau_nqm)\n",
    "        delta_tau_nqm = np.array(delta_tau_nqm)\n",
    "\n",
    "        if compute_uncertainties:\n",
    "            qr_med = QrMed(8 * W, s_weights, tau_nqm, delta_tau_nqm)\n",
    "            tau_dict[user_n] = qr_med\n",
    "            delta_tau_dict[user_n] = QrUnc(\n",
    "                8 * W, 1, s_weights, tau_nqm, delta_tau_nqm, qr_med=qr_med\n",
    "            )\n",
    "        else:\n",
    "            tau_dict[user_n] = BrMean(8 * W, s_weights, tau_nqm, delta_tau_nqm)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"s\": s_dict,\n",
    "            \"tau\": tau_dict,\n",
    "            **(\n",
    "                {\"delta_s\": delta_s_dict, \"delta_tau\": delta_tau_dict}\n",
    "                if compute_uncertainties\n",
    "                else {}\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "def get_scaling_for_supertrusted(ml_input: MlInput, individual_scores: pd.DataFrame):\n",
    "    rp = ml_input.get_ratings_properties()\n",
    "    rp.set_index([\"user_id\", \"entity_id\"], inplace=True)\n",
    "    rp = rp[rp.is_supertrusted]\n",
    "    df = individual_scores.join(rp, on=[\"user_id\", \"entity_id\"], how=\"inner\")\n",
    "    return compute_scaling(df, ml_input=ml_input)\n",
    "    \n",
    "def compute_scaled_scores(\n",
    "    ml_input: MlInput, individual_scores: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        - scaled individual scores: Dataframe with columns\n",
    "            * `user_id`\n",
    "            * `entity_id`\n",
    "            * `score`\n",
    "            * `uncertainty`\n",
    "            * `is_public`\n",
    "            * `is_trusted`\n",
    "            * `is_supertrusted`\n",
    "        - scalings: DataFrame with index `entity_id` and columns:\n",
    "            * `s`: scaling factor\n",
    "            * `tau`: translation value\n",
    "            * `delta_s`: uncertainty on `s`\n",
    "            * `delta_tau`: uncertainty on `tau`\n",
    "    \"\"\"\n",
    "    if len(individual_scores) == 0:\n",
    "        scores = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"user_id\",\n",
    "                \"entity_id\",\n",
    "                \"score\",\n",
    "                \"uncertainty\",\n",
    "                \"is_public\",\n",
    "                \"is_trusted\",\n",
    "                \"is_supertrusted\",\n",
    "            ]\n",
    "        )\n",
    "        scalings = pd.DataFrame(columns=[\"s\", \"tau\", \"delta_s\", \"delta_tau\"])\n",
    "        return scores, scalings\n",
    "\n",
    "    supertrusted_scaling = get_scaling_for_supertrusted(ml_input, individual_scores)\n",
    "    rp = ml_input.get_ratings_properties()\n",
    "\n",
    "    non_supertrusted_users = rp[\"user_id\"][~rp.is_supertrusted].unique()\n",
    "    supertrusted_users = rp[\"user_id\"][rp.is_supertrusted].unique()\n",
    "\n",
    "    rp.set_index([\"user_id\", \"entity_id\"], inplace=True)\n",
    "    df = individual_scores.join(rp, on=[\"user_id\", \"entity_id\"], how=\"left\")\n",
    "    df[\"is_public\"].fillna(False, inplace=True)\n",
    "    df[\"is_trusted\"].fillna(False, inplace=True)\n",
    "    df[\"is_supertrusted\"].fillna(False, inplace=True)\n",
    "\n",
    "    df = df.join(supertrusted_scaling, on=\"user_id\")\n",
    "    df[\"s\"].fillna(1, inplace=True)\n",
    "    df[\"tau\"].fillna(0, inplace=True)\n",
    "    df[\"score\"] = df[\"s\"] * df[\"score\"] + df[\"tau\"]\n",
    "    df[\"uncertainty\"] *= df[\"s\"]\n",
    "    df.drop([\"s\", \"tau\"], axis=1, inplace=True)\n",
    "    \n",
    "    non_supertrusted_scaling = compute_scaling(\n",
    "        df,\n",
    "        ml_input=ml_input,\n",
    "        users_to_compute=non_supertrusted_users,\n",
    "        reference_users=supertrusted_users,\n",
    "        compute_uncertainties=True,\n",
    "    )\n",
    "\n",
    "    df = df.join(non_supertrusted_scaling, on=\"user_id\")\n",
    "    df[\"s\"].fillna(1, inplace=True)\n",
    "    df[\"tau\"].fillna(0, inplace=True)\n",
    "    df[\"delta_s\"].fillna(0, inplace=True)\n",
    "    df[\"delta_tau\"].fillna(0, inplace=True)\n",
    "    df[\"uncertainty\"] = (\n",
    "        df[\"s\"] * df[\"uncertainty\"]\n",
    "        + df[\"delta_s\"] * df[\"score\"].abs()\n",
    "        + df[\"delta_tau\"]\n",
    "    )\n",
    "    df[\"score\"] = df[\"score\"] * df[\"s\"] + df[\"tau\"]\n",
    "    df.drop([\"s\", \"tau\", \"delta_s\", \"delta_tau\"], axis=1, inplace=True)\n",
    "\n",
    "    all_scalings = pd.concat([supertrusted_scaling, non_supertrusted_scaling])\n",
    "    return df, all_scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_MAX = 10  # Maximum score for a comparison in the input\n",
    "ALPHA = 0.01  # Signal-to-noise hyperparameter\n",
    "def compute_individual_score(scores: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Computation of contributor scores and score uncertainties,\n",
    "    based on their comparisons.\n",
    "\n",
    "    At this stage, scores will not be normalized between contributors.\n",
    "    \"\"\"\n",
    "    scores = scores[[\"entity_a\", \"entity_b\", \"score\"]]\n",
    "    scores_sym = pd.concat(\n",
    "        [\n",
    "            scores,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"entity_a\": scores.entity_b,\n",
    "                    \"entity_b\": scores.entity_a,\n",
    "                    \"score\": -1 * scores.score,\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # \"Comparison tensor\": matrix with all comparisons, values in [-R_MAX, R_MAX]\n",
    "    r = scores_sym.pivot(index=\"entity_a\", columns=\"entity_b\", values=\"score\")\n",
    "\n",
    "    r_tilde = r / (1.0 + R_MAX)\n",
    "    r_tilde2 = r_tilde ** 2\n",
    "\n",
    "    # r.loc[a:b] is negative when a is prefered to b.\n",
    "    l = -1.0 * r_tilde / np.sqrt(1.0 - r_tilde2)  # noqa: E741\n",
    "    k = (1.0 - r_tilde2) ** 3\n",
    "\n",
    "    L = k.mul(l).sum(axis=1)\n",
    "    K_diag = pd.DataFrame(\n",
    "        data=np.diag(k.sum(axis=1) + ALPHA),\n",
    "        index=k.index,\n",
    "        columns=k.index,\n",
    "    )\n",
    "    K = K_diag.sub(k, fill_value=0)\n",
    "\n",
    "    # theta_star = K^-1 * L\n",
    "    theta_star_numpy = np.linalg.solve(K, L)\n",
    "    theta_star = pd.Series(theta_star_numpy, index=L.index)\n",
    "\n",
    "    # Compute uncertainties\n",
    "    theta_star_ab = pd.DataFrame(\n",
    "        np.subtract.outer(theta_star_numpy, theta_star_numpy),\n",
    "        index=theta_star.index,\n",
    "        columns=theta_star.index,\n",
    "    )\n",
    "    sigma2 = (1.0 + (np.nansum(k * (l - theta_star_ab) ** 2) / 2)) / len(scores)\n",
    "    delta_star = pd.Series(np.sqrt(sigma2) / np.sqrt(np.diag(K)), index=K.index)\n",
    "\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            \"score\": theta_star,\n",
    "            \"uncertainty\": delta_star,\n",
    "        }\n",
    "    )\n",
    "    result.index.name = \"entity_id\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_individual_scores(\n",
    "    ml_input: MlInput, criteria: str, single_user_id: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    comparisons_df = ml_input.get_comparisons(criteria=criteria, user_id=single_user_id)\n",
    "\n",
    "    individual_scores = []\n",
    "    for (user_id, user_comparisons) in comparisons_df.groupby(\"user_id\"):\n",
    "        scores = compute_individual_score(user_comparisons)\n",
    "        if scores is None:\n",
    "            continue\n",
    "        scores[\"user_id\"] = user_id\n",
    "        individual_scores.append(scores.reset_index())\n",
    "\n",
    "    if len(individual_scores) == 0:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"entity_id\", \"score\", \"uncertainty\"])\n",
    "\n",
    "    result = pd.concat(individual_scores, ignore_index=True, copy=False)\n",
    "    return result[[\"user_id\", \"entity_id\", \"score\", \"uncertainty\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _run_mehestan_for_criterion(criteria: str, ml_input: MlInput):\n",
    "    indiv_scores = get_individual_scores(ml_input, criteria)\n",
    "    scaled_scores, scalings = compute_scaled_scores(\n",
    "        ml_input, individual_scores=indiv_scores\n",
    "    )\n",
    "    indiv_scores[\"criteria\"] = criteria\n",
    "    for mode in ScoreMode:\n",
    "        global_scores = get_global_scores(scaled_scores, score_mode=mode)\n",
    "        global_scores[\"criteria\"] = criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mehestan(ml_input: MlInput):\n",
    "    list_criteria=[\n",
    "    'largely_recommended',\n",
    "\t'reliability',\n",
    "\t'pedagogy',\n",
    "\t'importance',\t\n",
    "\t'layman_friendly',\t\n",
    "\t'entertaining_relaxing',\t\n",
    "\t'engaging',\t\n",
    "\t'diversity_inclusion',\t\n",
    "\t'better_habits',\t\n",
    "\t'backfire_risk',\n",
    "    ]\n",
    "    for criteria in list_criteria:\n",
    "        _run_mehestan_for_criterion(criteria,ml_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_input=MlInputFromPublicDataset(\"tournesol_public_export.csv\")\n",
    "run_mehestan(ml_input)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
